"""
    ecm(Y::JArray{Float64,2}, p::Int64, Œª::Number, Œ±::Number, Œ≤::Number; tol::Float64=1e-3, max_iter::Int64=1000, prerun::Int64=2, verb=true)

Estimate an elastic-net VAR(p) using the ECM algorithm in Pellegrino (2019).

# Arguments
- `Y`: observed measurements (`nxT`), where `n` and `T` are the number of series and observations.
- `p`: number of lags in the vector autoregression
- `Œª`: overall shrinkage hyper-parameter for the elastic-net penalty
- `Œ±`: weight associated to the LASSO component of the elastic-net penalty
- `Œ≤`: additional shrinkage for distant lags (p>1)
- `tol`: tolerance used to check convergence (default: 1e-3)
- `max_iter`: maximum number of iterations for the estimation algorithm (default: 1000)
- `prerun`: number of iterations prior the actual ECM estimation routine (default: 2)
- `verb`: Verbose output (default: true)

# References
Pellegrino (2019)
"""
function ecm(Y::JArray{Float64,2}, p::Int64, Œª::Number, Œ±::Number, Œ≤::Number; tol::Float64=1e-3, max_iter::Int64=1000, prerun::Int64=2, verb=true)

    #=
    -----------------------------------------------------------------------------------------------------------------------------------------------------
    Settings
    -----------------------------------------------------------------------------------------------------------------------------------------------------
    =#

    # Check hyper-parameters
    if Œ≤ < 1
        error("Œ≤ ‚â• 1");
    end

    if Œ± < 0 || Œ± > 1
        error("0 ‚â§ Œ± ‚â§ 1");
    end

    if Œª < 0
        error("Œª ‚â• 0");
    end

    # Check init_iter
    if max_iter < 3
        error("max_iter > 2");
    end

    if prerun >= max_iter
        error("prerun < max_iter");
    end

    # Dimensions
    n, T = size(Y);
    np = n*p;
    if n < 2
        error("This code is not compatible with univariate autoregressions");
    end

    # Œµ
    Œµ = 1e-8;

    # Gamma matrix
    Œì = [];
    for i=0:p-1
        if i == 0
            Œì = ones(n);
        else
            Œì = vcat(Œì, (Œ≤^i).*ones(n));
        end
    end
    Œì = Diagonal(Œª.*Œì);


    #=
    -----------------------------------------------------------------------------------------------------------------------------------------------------
    ECM initialisation
    -----------------------------------------------------------------------------------------------------------------------------------------------------
    =#

    # Interpolated data (used for the initialisation only)
    Y_init = copy(Y);
    for i=1:n
        Y_init[i, ismissing.(Y_init[i, :])] .= mean_skipmissing(Y_init[i, :]);
    end
    Y_init = Y_init |> Array{Float64};

    # Initialise using the coordinate descent algorithm
    if verb == true
        println("ecm > initialisation");
    end
    Œ®ÃÇ_init, Œ£ÃÇ_init = coordinate_descent(Y_init, p, Œª, Œ±, Œ≤, tol=tol, max_iter=max_iter, verb=false);

    save("./init_old.jld", Dict("Œ®_init" => Œ®ÃÇ_init, "Œ£_init" => Œ£ÃÇ_init));

    #=
    -----------------------------------------------------------------------------------------------------------------------------------------------------
    Memory pre-allocation
    -----------------------------------------------------------------------------------------------------------------------------------------------------
    =#

    #=
    The state vector includes additional n terms with respect to the standard VAR companion form representation.
    This is to estimate the lag-one covariance smoother as in Watson and Engle (1983).
    =#

    # State-space parameters
    BÃÇ = [Matrix{Float64}(I, n, n) zeros(n, np)];
    RÃÇ = Matrix{Float64}(I, n, n).*Œµ;
    CÃÇ, VÃÇ = ext_companion_form(Œ®ÃÇ_init, Œ£ÃÇ_init);

    # Initial conditions
    ùîõ0ÃÇ = zeros(np+n);
    P0ÃÇ = reshape((I-kron(CÃÇ, CÃÇ))\VÃÇ[:], np+n, np+n);
    P0ÃÇ = sym(P0ÃÇ);

    # Initialise additional variables
    Œ®ÃÇ = CÃÇ[1:n, 1:np];
    Œ£ÃÇ = VÃÇ[1:n, 1:n];
    Œ¶ÃÇ·µè = 1 ./ (abs.(Œ®ÃÇ).+Œµ);

    # ECM controls
    pen_loglik_old = -Inf;
    pen_loglik_new = -Inf;


    #=
    -----------------------------------------------------------------------------------------------------------------------------------------------------
    ECM algorithm
    -----------------------------------------------------------------------------------------------------------------------------------------------------
    =#

    # Run ECM
    for iter=1:max_iter

        # Run Kalman filter and smoother
        ùîõsÃÇ, PsÃÇ, _, ùîõs_0ÃÇ, Ps_0ÃÇ, _, _, _, loglik = kalman(Y, BÃÇ, RÃÇ, CÃÇ, VÃÇ, ùîõ0ÃÇ, P0ÃÇ; loglik_flag=true);

        if iter > prerun

            # New penalised loglikelihood
            pen_loglik_new = loglik - 0.5*tr(sym_inv(Œ£ÃÇ)*((1-Œ±).*sym(Œ®ÃÇ*Œì*Œ®ÃÇ') + Œ±.*sym((Œ®ÃÇ.*sqrt.(Œ¶ÃÇ·µè))*Œì*(Œ®ÃÇ.*sqrt.(Œ¶ÃÇ·µè))')));
            if verb == true
                println("ecm > iter=$(iter-prerun), penalised loglik=$(round(pen_loglik_new, digits=5))");
            end

            # Stop when the ECM algorithm converges
            if iter > prerun+1
                if (pen_loglik_new-pen_loglik_old)./(abs(pen_loglik_old)+Œµ) <= tol
                    if verb == true
                        println("ecm > converged!");
                        println("");
                    end
                    break;
                end
            end

            # Store current run information
            pen_loglik_old = copy(pen_loglik_new);

        elseif verb == true
            println("ecm > prerun $iter (out of $prerun)");
        end

        # Initial conditions
        ùîõ0ÃÇ = copy(ùîõs_0ÃÇ);
        P0ÃÇ = copy(Ps_0ÃÇ);

        # ECM statistics
        EÃÇ = zeros(n, n);
        FÃÇ = zeros(n, np);
        GÃÇ = zeros(np, np);

        for t=1:T
            EÃÇ += ùîõsÃÇ[1:n,t]*ùîõsÃÇ[1:n,t]' + PsÃÇ[1:n,1:n,t];

            if t == 1
                FÃÇ += ùîõsÃÇ[1:n,t]*ùîõ0ÃÇ[1:np]' + PsÃÇ[1:n,n+1:end,t];
                GÃÇ += ùîõ0ÃÇ[1:np]*ùîõ0ÃÇ[1:np]' + P0ÃÇ[1:np,1:np];

            else
                FÃÇ += ùîõsÃÇ[1:n,t]*ùîõsÃÇ[1:np,t-1]' + PsÃÇ[1:n,n+1:end,t];
                GÃÇ += ùîõsÃÇ[1:np,t-1]*ùîõsÃÇ[1:np,t-1]' + PsÃÇ[1:np,1:np,t-1];
            end
        end

        # VAR(p) coefficients
        Œ¶ÃÇ·µè = 1 ./ (abs.(Œ®ÃÇ).+Œµ);
        for i=1:n
            CÃÇ[i, 1:np] = sym_inv(GÃÇ + Œì.*((1-Œ±)*I + Œ±.*Diagonal(Œ¶ÃÇ·µè[i, :])))*FÃÇ[i,:];
        end

        # Update Œ®ÃÇ
        Œ®ÃÇ = CÃÇ[1:n, 1:np];

        # Covariance matrix of the VAR(p) residuals
        VÃÇ[1:n, 1:n] = sym(EÃÇ-FÃÇ*Œ®ÃÇ'-Œ®ÃÇ*FÃÇ'+Œ®ÃÇ*GÃÇ*Œ®ÃÇ') + (1-Œ±).*sym(Œ®ÃÇ*Œì*Œ®ÃÇ') + Œ±.*sym((Œ®ÃÇ.*sqrt.(Œ¶ÃÇ·µè))*Œì*(Œ®ÃÇ.*sqrt.(Œ¶ÃÇ·µè))');
        VÃÇ[1:n, 1:n] *= 1/T;

        # Update Œ£ÃÇ
        Œ£ÃÇ = VÃÇ[1:n, 1:n];

        #=
        save("./first_round_old.jld", Dict("Œ¶" => Œ¶ÃÇ·µè, "C" => CÃÇ, "V" => VÃÇ, "E" => EÃÇ, "F" => FÃÇ, "G" => GÃÇ));

        if iter == 2
            save("./second_round_old.jld", Dict("Œ¶" => Œ¶ÃÇ·µè, "C" => CÃÇ, "V" => VÃÇ, "E" => EÃÇ, "F" => FÃÇ, "G" => GÃÇ));
            error("");
        end
        =#
    end

    # The output excludes the additional n terms required to estimate the lag-one covariance smoother as described above.
    return BÃÇ[:,1:np], RÃÇ, CÃÇ[1:np,1:np], VÃÇ[1:np,1:np], ùîõ0ÃÇ[1:np], P0ÃÇ[1:np,1:np], Œ®ÃÇ_init, Œ£ÃÇ_init;
end
