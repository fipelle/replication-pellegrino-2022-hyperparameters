"""
    ecm(Y::JArray{Float64,2}, p::Int64, Î»::Number, Î±::Number, Î²::Number; tol::Float64=1e-4, max_iter::Int64=1000, prerun::Int64=2, verb=true)

Estimate an elastic-net VAR(p) using the ECM algorithm in Pellegrino (2019).

# Arguments
- `Y`: observed measurements (`nxT`), where `n` and `T` are the number of series and observations.
- `p`: number of lags in the vector autoregression
- `Î»`: overall shrinkage hyper-parameter for the elastic-net penalty
- `Î±`: weight associated to the LASSO component of the elastic-net penalty
- `Î²`: additional shrinkage for distant lags (p>1)
- `tol`: tolerance used to check convergence (default: 1e-4)
- `max_iter`: maximum number of iterations for the estimation algorithm (default: 1000)
- `prerun`: number of iterations prior the actual ECM estimation routine (default: 2)
- `verb`: Verbose output (default: true)

# References
Pellegrino (2019)
"""
function ecm(Y::JArray{Float64,2}, p::Int64, Î»::Number, Î±::Number, Î²::Number; tol::Float64=1e-4, max_iter::Int64=1000, prerun::Int64=2, verb=true)

    #=
    -----------------------------------------------------------------------------------------------------------------------------------------------------
    Settings
    -----------------------------------------------------------------------------------------------------------------------------------------------------
    =#

    # Check hyper-parameters
    if Î² < 1
        error("Î² â‰¥ 1");
    end

    if Î± < 0 || Î± > 1
        error("0 â‰¤ Î± â‰¤ 1");
    end

    if Î» < 0
        error("Î» â‰¥ 0");
    end

    # Check init_iter
    if max_iter < 3
        error("max_iter > 2");
    end

    if prerun >= max_iter
        error("prerun < max_iter");
    end

    # Dimensions
    n, T = size(Y);
    np = n*p;
    if n < 2
        error("This code is not compatible with univariate autoregressions");
    end

    # Îµ
    Îµ = 1e-8;

    # Gamma matrix
    Î“ = [];
    for i=0:p-1
        if i == 0
            Î“ = Matrix{Float64}(I, n, n);
        else
            Î“ = cat(Î“, (Î²^i).*Matrix{Float64}(I, n, n), dims=[1,2]);
        end
    end
    Î“ = Î».*Î“;


    #=
    -----------------------------------------------------------------------------------------------------------------------------------------------------
    ECM initialisation
    -----------------------------------------------------------------------------------------------------------------------------------------------------
    =#

    # Interpolated data (used for the initialisation only)
    Y_init = copy(Y);
    for i=1:n
        Y_init[i, ismissing.(Y_init[i, :])] .= mean_skipmissing(Y_init[i, :]);
    end
    Y_init = Y_init |> Array{Float64};

    # Initialise using the coordinate descent algorithm
    if verb == true
        println("ecm > initialisation");
    end
    Î¨Ì‚_init, Î£Ì‚_init = coordinate_descent(Y_init, p, Î», Î±, Î², tol=tol, max_iter=max_iter, verb=false);


    #=
    -----------------------------------------------------------------------------------------------------------------------------------------------------
    Memory pre-allocation
    -----------------------------------------------------------------------------------------------------------------------------------------------------
    =#

    #=
    The state vector includes additional n terms with respect to the standard VAR companion form representation.
    This is to estimate the lag-one covariance smoother as in Watson and Engle (1983).
    =#

    # State-space parameters
    BÌ‚ = [Matrix{Float64}(I, n, n) zeros(n, np)];
    RÌ‚ = Matrix{Float64}(I, n, n).*Îµ;
    CÌ‚, VÌ‚ = ext_companion_form(Î¨Ì‚_init, Î£Ì‚_init);

    # Initial conditions
    ğ”›0Ì‚ = zeros(np+n);
    P0Ì‚ = reshape((Matrix(I, (np+n)^2, (np+n)^2)-kron(CÌ‚, CÌ‚))\VÌ‚[:], np+n, np+n);
    P0Ì‚ = sym(P0Ì‚);

    # Initialise additional variables
    Î¨Ì‚ = CÌ‚[1:n, 1:np];
    Î£Ì‚ = VÌ‚[1:n, 1:n];
    Î¦Ì‚áµ = 1 ./ (abs.(Î¨Ì‚).+Îµ);

    # ECM controls
    pen_loglik_old = -Inf;
    pen_loglik_new = -Inf;


    #=
    -----------------------------------------------------------------------------------------------------------------------------------------------------
    ECM algorithm
    -----------------------------------------------------------------------------------------------------------------------------------------------------
    =#

    # Run ECM
    for iter=1:max_iter

        # Run Kalman filter and smoother
        ğ”›sÌ‚, PsÌ‚, _, ğ”›s_0Ì‚, Ps_0Ì‚, _, _, _, loglik = kalman(Y, BÌ‚, RÌ‚, CÌ‚, VÌ‚, ğ”›0Ì‚, P0Ì‚; loglik_flag=true);

        if iter > prerun

            # New penalised loglikelihood
            pen_loglik_new = loglik - 0.5*tr(sym_inv(Î£Ì‚)*((1-Î±).*Î¨Ì‚ + Î±.*(Î¨Ì‚.*Î¦Ì‚áµ))*Î“*Î¨Ì‚');

            if verb == true
                println("ecm > iter=$(iter-prerun), penalised loglik=$(round(pen_loglik_new, digits=5))");
            end

            # Stop when the ECM algorithm converges
            if iter > prerun+1
                if (pen_loglik_new-pen_loglik_old)./(abs(pen_loglik_old)+Îµ) <= tol
                    if verb == true
                        println("ecm > converged!");
                        println("");
                    end
                    break;
                end
            end

            # Store current run information
            pen_loglik_old = copy(pen_loglik_new);

        elseif verb == true
            println("ecm > prerun $iter (out of $prerun)");
        end

        # Initial conditions
        ğ”›0Ì‚ = copy(ğ”›s_0Ì‚);
        P0Ì‚ = copy(Ps_0Ì‚);

        # ECM statistics
        EÌ‚ = zeros(n, n);
        FÌ‚ = zeros(n, np);
        GÌ‚ = zeros(np, np);

        for t=1:T
            EÌ‚ += ğ”›sÌ‚[1:n,t]*ğ”›sÌ‚[1:n,t]' + PsÌ‚[1:n,1:n,t];

            if t == 1
                FÌ‚ += ğ”›sÌ‚[1:n,t]*ğ”›0Ì‚[1:np]' + PsÌ‚[1:n,n+1:end,t];
                GÌ‚ += ğ”›0Ì‚[1:np]*ğ”›0Ì‚[1:np]' + P0Ì‚[1:np,1:np];

            else
                FÌ‚ += ğ”›sÌ‚[1:n,t]*ğ”›sÌ‚[1:np,t-1]' + PsÌ‚[1:n,n+1:end,t];
                GÌ‚ += ğ”›sÌ‚[1:np,t-1]*ğ”›sÌ‚[1:np,t-1]' + PsÌ‚[1:np,1:np,t-1];
            end
        end

        # VAR(p) coefficients
        Î¦Ì‚áµ = 1 ./ (abs.(Î¨Ì‚).+Îµ);
        for i=1:n
            CÌ‚[i, 1:np] = sym_inv(GÌ‚ + Î“.*((1-Î±)*I + Î±.*Diagonal(Î¦Ì‚áµ[i, :])))*FÌ‚[i,:];
        end

        # Update Î¨Ì‚
        Î¨Ì‚ = CÌ‚[1:n, 1:np];

        # Covariance matrix of the VAR(p) residuals
        VÌ‚[1:n, 1:n] = sym(EÌ‚-FÌ‚*Î¨Ì‚'-Î¨Ì‚*FÌ‚'+Î¨Ì‚*GÌ‚*Î¨Ì‚' + Î¨Ì‚*Î“*((1-Î±).*Î¨Ì‚ + Î±.*Î¨Ì‚.*Î¦Ì‚áµ)')./T;

        # Update Î£Ì‚
        Î£Ì‚ = VÌ‚[1:n, 1:n];
    end

    # The output excludes the additional n terms required to estimate the lag-one covariance smoother as described above.
    return BÌ‚[:,1:np], RÌ‚, CÌ‚[1:np,1:np], VÌ‚[1:np,1:np], ğ”›0Ì‚[1:np], P0Ì‚[1:np,1:np], Î¨Ì‚_init, Î£Ì‚_init;
end
